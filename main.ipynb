{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O04pkr6Z0j4G"
      },
      "source": [
        "#%%capture\n",
        "print('NOTE: Intentionally crashing session to use the newly installed library.\\n')\n",
        "!pip uninstall -y pyarrow\n",
        "!pip install ray[debug]==0.7.5\n",
        "# A hack to force the runtime to restart, needed to include the above dependencies.\n",
        "import os\n",
        "os._exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDO2pwCsk3mU"
      },
      "source": [
        "import os\n",
        "print(os.popen('ls').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AGqSTWYk3mU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "3b653b67-3de5-4946-cb97-d08454194b44"
      },
      "source": [
        "print(os.getcwd())\n",
        "print(os.system(\"pwd\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f15d34457471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pwd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO54Kb6mk3mU",
        "outputId": "f65de508-4e77-4ba7-a39a-487ba6575f08"
      },
      "source": [
        "!pip3 install termios"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement termios\n",
            "ERROR: No matching distribution found for termios\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68wgJkDehIXJ"
      },
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzq9OclK0pg5"
      },
      "source": [
        "%%capture\n",
        "!pip install python-Levenshtein\n",
        "!pip install redis\n",
        "!pip install -U ray\n",
        "!pip install ray[debug]==0.7.5\n",
        "!pip install ray[rllib]  # also recommended: ray[debug]\n",
        "!pip uninstall -y pyarrow\n",
        "!pip install unicodedata2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBUU7M16k70v"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSDz6m3g55L9"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LedW5b8H0mki",
        "outputId": "b82f3294-28c8-42f7-eaf9-5256e414f00e"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H1vSh2nH0rST",
        "outputId": "c0880553-703e-490b-b9c7-9692c5d82bdf"
      },
      "source": [
        "import os\n",
        "# os.chdir(\"/content/drive/My Drive/Knowledge Extraction/szhang37_code/KG_RL\") ## change to current folder\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import random\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import unicodedata\n",
        "from functools import reduce\n",
        "#### Import ray related package\n",
        "import ray\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n",
        "\n",
        "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
        "from ray.rllib.models.tf.misc import normc_initializer\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.agents.dqn.distributional_q_model import DistributionalQModel\n",
        "from ray.rllib.models.tf.visionnet_v2 import VisionNetwork as MyVisionNetwork\n",
        "from ray.tune.logger import pretty_print\n",
        "from ray.rllib.utils import try_import_tf\n",
        "from ray.tune import grid_search\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.tune import Trainable\n",
        "from ray.tune.logger import pretty_print\n",
        "from ray.tune import run as run_tune\n",
        "from ray.tune.registry import register_env\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.spaces import Discrete, Box\n",
        "from ray import tune\n",
        "from ray.rllib.agents.dqn.dqn import DQNTrainer, DEFAULT_CONFIG\n",
        "### import self-defined function\n",
        "import similarity_metrics\n",
        "from utility_function import *\n",
        "### import Environment\n",
        "from Environment import KGRLEnv\n",
        "### Import Customized DQN\n",
        "from PolicyDQN import *\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI1RfQp50aTP"
      },
      "source": [
        "## Read processed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RjVvaunqTF7"
      },
      "source": [
        "method = \"bert\" ## possible methods: \"bert\", \"bidaf\", \"qanet\"\n",
        "########################################\n",
        "## read preprocessed data(train/test) ##\n",
        "########################################\n",
        "train_data = pickle.load(open(\"drive/MyDrive/preprocessed_data/train_data.pkl\", \"rb\" ))\n",
        "test_data = pickle.load(open(\"drive/MyDrive/preprocessed_data/test_data.pkl\", \"rb\" ))\n",
        "test_data_types = [test_data[i]['type'] for i in range(len(test_data))]\n",
        "\n",
        "do_bert = False\n",
        "if method == \"bert\":\n",
        "  do_bert = True\n",
        "#######################################################\n",
        "### Obtain the pickled predition for train and test ###\n",
        "#######################################################\n",
        "saving_path =\"drive/MyDrive/preprocessed_data/\"\n",
        "train_file = \"pred_train_%s.pkl\" %method ; test_file = \"pred_test_%s.pkl\" %method\n",
        "pred_train = pickle.load(open(saving_path + train_file, \"rb\" ))\n",
        "pred_test = pickle.load(open(saving_path + test_file, \"rb\" ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5RQJb6ESWqq"
      },
      "source": [
        "## Santity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wonczRbzhzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5a6706-c87d-44fb-c1ed-8a745453c2db"
      },
      "source": [
        "##################\n",
        "## santity check #\n",
        "##################\n",
        "## check the length of pickled answer\n",
        "for k,v in pred_train.items():\n",
        "  assert len(v) == 10, \"%s does not match\" %k\n",
        "\n",
        "for k,v in pred_test.items():\n",
        "  assert len(v) == 10, \"%s does not match\" %k\n",
        "\n",
        "#### check the length of example_similarity_features\n",
        "ans = '65nm'\n",
        "reference_values = ['16nm FinFET', '16nm FinFET', '14nm FinFET', '28纳米', '65nm',\n",
        "        '28纳米', '16nm FinFET']\n",
        "example_similarity_features = get_sim_features(ans, reference_values, do_bert = do_bert)\n",
        "len_similarity_features = len(example_similarity_features)\n",
        "print(len_similarity_features)\n",
        "print(example_similarity_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 6.0, 0.5142857142857143, 0.5142857142857143, 0.2857142857142857, 0.2857142857142857, 0.2571428571428571, 0.3, 3.4285714285714284]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jtxcAwN1Oxo"
      },
      "source": [
        "## Baseline Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnitp0JG1ODM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00888a13-3d80-410a-9416-e2b703006a1c"
      },
      "source": [
        "print(\"+\" * 30 + \" For method %s, the scores are \" %method + \"+\" * 30)\n",
        "avg_score = []\n",
        "oracle_score = []\n",
        "max_conf_score = []\n",
        "majority_vote_score = []\n",
        "first_score = []\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "for i in range(len(test_data)):\n",
        "  cur = pred_test['Test_'+str(i)]\n",
        "  candidate_answer = [cur[j][0][0] for j in range(len(cur))]\n",
        "  confs = [cur[j][1][0] for j in range(len(cur))]\n",
        "  if do_bert: ## if do bert the object should be preprocessed as original bert paper\n",
        "    sims = [similarity_metrics.LevenSim(c, token_word(test_data[i]['o']) ) for c in candidate_answer]\n",
        "  else:\n",
        "    sims = [similarity_metrics.LevenSim(c, test_data[i]['o']) for c in candidate_answer]\n",
        "  # majority vote\n",
        "  c = Counter(candidate_answer)\n",
        "  ans_majority_vote, _ = c.most_common()[0]\n",
        "  first_score.append(sims[0])\n",
        "  avg_score.append(np.mean(sims))\n",
        "  max_conf_score.append(sims[np.argmax(confs)])\n",
        "  majority_vote_score.append(similarity_metrics.LevenSim(ans_majority_vote, \n",
        "                                                         test_data[i]['o']))\n",
        "  oracle_score.append(np.max(sims))\n",
        "\n",
        "test_data_types = [test_data[i]['type'] for i in range(len(test_data))]\n",
        "GPU_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'GPUs']\n",
        "Game_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'Games']\n",
        "Movie_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'Movies']\n",
        "Phone_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'phones']\n",
        "\n",
        "## print the scores \n",
        "\n",
        "print(\"first score is %0.3f (Overall)\"%np.mean(np.array(first_score)))\n",
        "print(\"avg score is %0.3f (Overall)\"%np.mean(np.array(avg_score)))\n",
        "print(\"mac conf score is %0.3f (Overall)\"%np.mean(np.array(max_conf_score)))\n",
        "print(\"majority vote score is %0.3f (Overall)\"%np.mean(np.array(majority_vote_score)))\n",
        "print(\"oracle score is %0.3f (Overall)\"%np.mean(np.array(oracle_score)))\n",
        "\n",
        "index_to_use = GPU_test_index\n",
        "print(\"first score is %0.3f for GPU\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for GPU\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for GPU\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for GPU\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for GPU\"%np.mean(np.array(oracle_score)[index_to_use]))\n",
        "\n",
        "index_to_use = Game_test_index\n",
        "print(\"first score is %0.3f for GAME\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for GAME\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for GAME\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for GAME\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for GAME\"%np.mean(np.array(oracle_score)[index_to_use]))\n",
        "\n",
        "index_to_use = Movie_test_index\n",
        "print(\"first score is %0.3f for MOVIE\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for MOVIE\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for MOVIE\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for MOVIE\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for MOVIE\"%np.mean(np.array(oracle_score)[index_to_use]))\n",
        "\n",
        "index_to_use = Phone_test_index\n",
        "print(\"first score is %0.3f for PHONE\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for PHONE\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for PHONE\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for PHONE\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for PHONE\"%np.mean(np.array(oracle_score)[index_to_use]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++++++++++++ For method bert, the scores are ++++++++++++++++++++++++++++++\n",
            "first score is 0.642 (Overall)\n",
            "avg score is 0.314 (Overall)\n",
            "mac conf score is 0.605 (Overall)\n",
            "majority vote score is 0.492 (Overall)\n",
            "oracle score is 0.895 (Overall)\n",
            "first score is 0.581 for GPU\n",
            "avg score is 0.374 for GPU\n",
            "mac conf score is 0.727 for GPU\n",
            "majority vote score is 0.502 for GPU\n",
            "oracle score is 0.925 for GPU\n",
            "first score is 0.579 for GAME\n",
            "avg score is 0.234 for GAME\n",
            "mac conf score is 0.600 for GAME\n",
            "majority vote score is 0.394 for GAME\n",
            "oracle score is 0.857 for GAME\n",
            "first score is 0.670 for MOVIE\n",
            "avg score is 0.361 for MOVIE\n",
            "mac conf score is 0.552 for MOVIE\n",
            "majority vote score is 0.625 for MOVIE\n",
            "oracle score is 0.887 for MOVIE\n",
            "first score is 0.738 for PHONE\n",
            "avg score is 0.287 for PHONE\n",
            "mac conf score is 0.540 for PHONE\n",
            "majority vote score is 0.449 for PHONE\n",
            "oracle score is 0.909 for PHONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a6CuDDp55dO"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25YrNMHp4pDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74afeac5-3931-4583-addc-b89d85634be1"
      },
      "source": [
        "seed = 20201015\n",
        "np.random.seed(seed)\n",
        "tf.random.set_random_seed(seed)\n",
        "ray.init(num_gpus=1, log_to_driver=False, local_mode=True, ignore_reinit_error=True)\n",
        "ModelCatalog.register_custom_model(\"keras_q_model\", DQNModel)\n",
        "\n",
        "qTrainer = DQNTrainer(env=KGRLEnv, config={# config to pass to env class\n",
        "    \"model\": {\n",
        "        \"custom_model\": \"keras_q_model\"\n",
        "    },\n",
        "    \"seed\" : seed,\n",
        "    \"env_config\": {\"training\": True, \"idx_to_test\":None, \"train_data\" : train_data,\"test_data\": test_data,\"pred_train\":  pred_train, \"pred_test\" : pred_test, \"do_bert\" : do_bert},\n",
        "    \"buffer_size\":100,\n",
        "    \"lr_schedule\": [[0, 0.05], [20, 0.01], [30, 0.005], [50, 0.001]],\n",
        "    \"train_batch_size\":100\n",
        "  })\n",
        "\n",
        "\n",
        "total_iteration = 20\n",
        "prev_time = time.time()\n",
        "for i in range(total_iteration):\n",
        "    print(\"iteration {};\".format(i), \\\n",
        "          \"%d sec/iteration;\" % (time.time()- prev_time), \\\n",
        "          \"%d min remaining\" % ((total_iteration - i)*(time.time()- prev_time)/60))\n",
        "    prev_time = time.time()\n",
        "    qTrainer.train()\n",
        "    \n",
        "print(\"Training Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-31 07:01:14,902\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
            "2021-03-31 07:01:29,315\tINFO rollout_worker.py:768 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f5c0db5b050>}\n",
            "2021-03-31 07:01:29,316\tINFO rollout_worker.py:769 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f5b54572fd0>}\n",
            "2021-03-31 07:01:29,318\tINFO rollout_worker.py:370 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f5b54572e50>}\n",
            "2021-03-31 07:01:29,325\tWARNING sync_replay_optimizer.py:105 -- buffer_size=100 < replay_starts=1000\n",
            "2021-03-31 07:01:29,326\tINFO trainable.py:102 -- _setup took 13.723 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2021-03-31 07:01:29,363\tINFO rollout_worker.py:467 -- Generating sample batch of size 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 0; 0 sec/iteration; 0 min remaining\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-31 07:01:29,861\tINFO sampler.py:310 -- Raw obs from env: { 0: { 'agent0': [ 1.0,\n",
            "                   0.2533668055974987,\n",
            "                   1.0,\n",
            "                   1.0,\n",
            "                   1.0,\n",
            "                   1.0,\n",
            "                   0.6,\n",
            "                   0.6,\n",
            "                   1.0,\n",
            "                   0.9,\n",
            "                   0.9,\n",
            "                   1.0,\n",
            "                   1.0,\n",
            "                   0.55,\n",
            "                   0.55,\n",
            "                   0.5,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   5.0,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   0.0,\n",
            "                   4.5,\n",
            "                   0]}}\n",
            "2021-03-31 07:01:29,862\tINFO sampler.py:311 -- Info return from env: {0: {'agent0': None}}\n",
            "2021-03-31 07:01:29,864\tINFO sampler.py:409 -- Preprocessed obs: [ 1.0,\n",
            "  0.2533668055974987,\n",
            "  1.0,\n",
            "  1.0,\n",
            "  1.0,\n",
            "  1.0,\n",
            "  0.6,\n",
            "  0.6,\n",
            "  1.0,\n",
            "  0.9,\n",
            "  0.9,\n",
            "  1.0,\n",
            "  1.0,\n",
            "  0.55,\n",
            "  0.55,\n",
            "  0.5,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  5.0,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  0.0,\n",
            "  4.5,\n",
            "  0]\n",
            "2021-03-31 07:01:29,865\tINFO sampler.py:413 -- Filtered obs: np.ndarray((31,), dtype=float64, min=0.0, max=5.0, mean=0.721)\n",
            "2021-03-31 07:01:29,868\tINFO sampler.py:528 -- Inputs to compute_actions():\n",
            "\n",
            "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "                                  'env_id': 0,\n",
            "                                  'info': None,\n",
            "                                  'obs': np.ndarray((31,), dtype=float64, min=0.0, max=5.0, mean=0.721),\n",
            "                                  'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "                                  'prev_reward': 0.0,\n",
            "                                  'rnn_state': []},\n",
            "                        'type': 'PolicyEvalData'}]}\n",
            "\n",
            "2021-03-31 07:01:29,871\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
            "2021-03-31 07:01:30,696\tINFO sampler.py:555 -- Outputs of compute_actions():\n",
            "\n",
            "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
            "                      [],\n",
            "                      { 'q_values': np.ndarray((1, 3), dtype=float32, min=0.012, max=0.052, mean=0.038)})}\n",
            "\n",
            "2021-03-31 07:01:30,698\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
            "\n",
            "{ 'agent0': { 'data': { 'actions': np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
            "                        'agent_index': np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "                        'dones': np.ndarray((1,), dtype=bool, min=1.0, max=1.0, mean=1.0),\n",
            "                        'eps_id': np.ndarray((1,), dtype=int64, min=1235374113.0, max=1235374113.0, mean=1235374113.0),\n",
            "                        'infos': np.ndarray((1,), dtype=object, head={'final_answer': ['32gb'], 'steps': 2}),\n",
            "                        'new_obs': np.ndarray((1, 31), dtype=float32, min=0.0, max=5.0, mean=0.721),\n",
            "                        'obs': np.ndarray((1, 31), dtype=float32, min=0.0, max=5.0, mean=0.721),\n",
            "                        'prev_actions': np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "                        'prev_rewards': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
            "                        'q_values': np.ndarray((1, 3), dtype=float32, min=0.012, max=0.052, mean=0.038),\n",
            "                        'rewards': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "                        't': np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "                        'unroll_id': np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "                        'weights': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
            "              'type': 'SampleBatch'}}\n",
            "\n",
            "2021-03-31 07:01:31,781\tINFO rollout_worker.py:501 -- Completed sample batch:\n",
            "\n",
            "{ 'data': { 'actions': np.ndarray((4,), dtype=int64, min=0.0, max=2.0, mean=1.25),\n",
            "            'agent_index': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "            'dones': np.ndarray((4,), dtype=bool, min=0.0, max=1.0, mean=0.5),\n",
            "            'eps_id': np.ndarray((4,), dtype=int64, min=832817057.0, max=1235374113.0, mean=933456321.0),\n",
            "            'infos': np.ndarray((4,), dtype=object, head={'final_answer': ['32gb'], 'steps': 2}),\n",
            "            'new_obs': np.ndarray((4, 31), dtype=float32, min=0.0, max=11.0, mean=1.131),\n",
            "            'obs': np.ndarray((4, 31), dtype=float32, min=0.0, max=13.0, mean=1.153),\n",
            "            'prev_actions': np.ndarray((4,), dtype=int64, min=0.0, max=1.0, mean=0.25),\n",
            "            'prev_rewards': np.ndarray((4,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
            "            'q_values': np.ndarray((4, 3), dtype=float32, min=-0.096, max=0.104, mean=0.022),\n",
            "            'rewards': np.ndarray((4,), dtype=float32, min=0.0, max=1.0, mean=0.281),\n",
            "            't': np.ndarray((4,), dtype=int64, min=0.0, max=2.0, mean=0.75),\n",
            "            'unroll_id': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "            'weights': np.ndarray((4,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
            "  'type': 'SampleBatch'}\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "2021-03-31 07:02:33,320\tINFO rollout_worker.py:591 -- Training on concatenated sample batches:\n",
            "\n",
            "{ 'count': 100,\n",
            "  'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((100,), dtype=int64, min=0.0, max=2.0, mean=0.87),\n",
            "                                                    'batch_indexes': np.ndarray((100,), dtype=int64, min=4.0, max=99.0, mean=52.11),\n",
            "                                                    'dones': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.21),\n",
            "                                                    'new_obs': np.ndarray((100, 31), dtype=float32, min=0.0, max=37.0, mean=1.241),\n",
            "                                                    'obs': np.ndarray((100, 31), dtype=float32, min=0.0, max=37.0, mean=1.244),\n",
            "                                                    'rewards': np.ndarray((100,), dtype=float64, min=0.0, max=1.0, mean=0.077),\n",
            "                                                    'weights': np.ndarray((100,), dtype=float64, min=1.0, max=1.0, mean=1.0)},\n",
            "                                          'type': 'SampleBatch'}},\n",
            "  'type': 'MultiAgentBatch'}\n",
            "\n",
            "2021-03-31 07:02:33,321\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(5, 256) dtype=float32_ref>\n",
            "2021-03-31 07:02:33,322\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
            "2021-03-31 07:02:33,324\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 3) dtype=float32_ref>\n",
            "2021-03-31 07:02:33,325\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(3,) dtype=float32_ref>\n",
            "2021-03-31 07:02:33,326\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(5, 256) dtype=float32_ref>\n",
            "2021-03-31 07:02:33,328\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
            "2021-03-31 07:02:33,329\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
            "2021-03-31 07:02:33,332\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
            "2021-03-31 07:02:33,335\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/my_layer1/kernel:0' shape=(31, 10) dtype=float32>\n",
            "2021-03-31 07:02:33,337\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/my_layer1/bias:0' shape=(10,) dtype=float32>\n",
            "2021-03-31 07:02:33,339\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/my_out/kernel:0' shape=(10, 5) dtype=float32>\n",
            "2021-03-31 07:02:33,340\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/my_out/bias:0' shape=(5,) dtype=float32>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1; 63 sec/iteration; 20 min remaining\n",
            "iteration 2; 45 sec/iteration; 13 min remaining\n",
            "iteration 3; 43 sec/iteration; 12 min remaining\n",
            "iteration 4; 41 sec/iteration; 11 min remaining\n",
            "iteration 5; 41 sec/iteration; 10 min remaining\n",
            "iteration 6; 41 sec/iteration; 9 min remaining\n",
            "iteration 7; 41 sec/iteration; 8 min remaining\n",
            "iteration 8; 46 sec/iteration; 9 min remaining\n",
            "iteration 9; 44 sec/iteration; 8 min remaining\n",
            "iteration 10; 43 sec/iteration; 7 min remaining\n",
            "iteration 11; 43 sec/iteration; 6 min remaining\n",
            "iteration 12; 44 sec/iteration; 5 min remaining\n",
            "iteration 13; 44 sec/iteration; 5 min remaining\n",
            "iteration 14; 46 sec/iteration; 4 min remaining\n",
            "iteration 15; 45 sec/iteration; 3 min remaining\n",
            "iteration 16; 40 sec/iteration; 2 min remaining\n",
            "iteration 17; 40 sec/iteration; 2 min remaining\n",
            "iteration 18; 43 sec/iteration; 1 min remaining\n",
            "iteration 19; 42 sec/iteration; 0 min remaining\n",
            "Training Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAC4IzIRBjrV"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7sk510J5Nmg",
        "outputId": "e2c3009d-c982-423f-9ee3-bcf2ac853a30"
      },
      "source": [
        "def evaluation_q(test_data, pred_test, qTrainer):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    for i in range(len(test_data)):\n",
        "      env = KGRLEnv({\"training\": False, \"idx_to_test\":i, \"train_data\" : None, \"test_data\": test_data,\"pred_train\":  None, \"pred_test\" : pred_test, \"do_bert\" : do_bert})\n",
        "      state = env.state\n",
        "      done = False\n",
        "      while not done:\n",
        "          action = qTrainer.compute_action(state)\n",
        "          state, reward, done, results = env.step(action)\n",
        "      rewards.append(reward)\n",
        "      steps.append(results['steps'])\n",
        "    return rewards, steps\n",
        "\n",
        "reward_list, step_list = evaluation_q(test_data, pred_test, qTrainer)\n",
        "avg_reward = np.mean(reward_list)\n",
        "avg_steps = np.mean(step_list)\n",
        "\n",
        "GPU_reward = np.mean(np.array(reward_list)[GPU_test_index])\n",
        "GPU_steps = np.mean(np.array(step_list)[GPU_test_index])\n",
        "\n",
        "Movie_reward = np.mean(np.array(reward_list)[Movie_test_index])\n",
        "Movie_steps = np.mean(np.array(step_list)[Movie_test_index])\n",
        "\n",
        "Game_reward = np.mean(np.array(reward_list)[Game_test_index])\n",
        "Game_steps = np.mean(np.array(step_list)[Game_test_index])\n",
        "\n",
        "Phone_reward = np.mean(np.array(reward_list)[Phone_test_index])\n",
        "Phone_steps = np.mean(np.array(step_list)[Phone_test_index])\n",
        "print(\"Training iteration {}..., \\n average reward is {:0.3f},\\\n",
        "average # of steps is {:0.3f}\".format(i, avg_reward, avg_steps))\n",
        "\n",
        "print(\"Average rewards for GPU/Movie/Game/Phone are {:0.3f}/{:0.3f}/{:0.3f}/{:0.3f}\"\\\n",
        "      .format(GPU_reward, Movie_reward, Game_reward, Phone_reward))\n",
        "\n",
        "print(\"Average # of steps for GPU/Movie/Game/Phone are {:0.3f}/{:0.3f}/{:0.3f}/{:0.3f}\"\\\n",
        "      .format(GPU_steps, Movie_steps, Game_steps, Phone_steps))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training iteration 19..., \n",
            " average reward is 0.748,average # of steps is 3.533\n",
            "Average rewards for GPU/Movie/Game/Phone are 0.744/0.758/0.681/0.808\n",
            "Average # of steps for GPU/Movie/Game/Phone are 3.493/3.307/4.187/3.147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9opqneN1YyF"
      },
      "source": [
        "# Ablation Study: part of KG are missing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0JQBlxZgf8s"
      },
      "source": [
        "use_percentage = 0.5 # control the percentage of data that can leverage KG\n",
        "\n",
        "train_kg_index = np.random.binomial(1, use_percentage, len(train_data))\n",
        "test_kg_index = np.random.binomial(1, use_percentage, len(test_data))\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "  train_data[i]['kg_index'] = train_kg_index[i]\n",
        "for i in range(len(test_data)):\n",
        "  test_data[i]['kg_index'] = test_kg_index[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aus8Prut1Xht",
        "outputId": "7220d02b-7dd9-49ca-b711-078812d371ae"
      },
      "source": [
        "class KGRLMask(gym.Env):\n",
        "    def _build_init(self, kg_dir = \"drive/MyDrive/related_triples_by_relation/\"):\n",
        "        \"\"\"\n",
        "        when testing, use idx_to_test, otherwise randomly sample a training data\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "          idx = np.random.choice(range(len(self.train_data))) \n",
        "          self.entry = self.train_data[idx]\n",
        "        else:\n",
        "          idx = self.idx_to_test\n",
        "          self.entry = self.test_data[idx]\n",
        "        self.query = self.entry['s'] + ' ' + self.entry['p']\n",
        "        self.text_list = self.entry['corpus']\n",
        "        ######################################################\n",
        "        ## obtain the answer from extraction system output ###\n",
        "        ######################################################\n",
        "        if self.training:\n",
        "          self.answer_list = pred_train[self.entry['id']] \n",
        "        else:\n",
        "          self.answer_list = pred_test[self.entry['id']]\n",
        "        self.text_answer = [[self.text_list[i], self.answer_list[i]] for i in range(len(self.text_list))]\n",
        "        self.max_index = len(self.text_list)\n",
        "        ### #####################################################################\n",
        "        ## initialize the index of current/new candidate as 0/1 respectively. ###\n",
        "        #########################################################################\n",
        "        self.cur_index = 0\n",
        "        self.new_index = 1\n",
        "        self.cur = self.text_answer[self.cur_index]\n",
        "        try:\n",
        "          self.new = self.text_answer[self.new_index]\n",
        "        except:\n",
        "          ####################################################################\n",
        "          ## exception would happen when size of raw text is less than 2. ####\n",
        "          ## which cannot happen in preprocessed data ########################\n",
        "          ####################################################################\n",
        "          self.new =  self.cur\n",
        "        self.curans = self.cur[1][0]\n",
        "        self.newans = self.new[1][0]\n",
        "        self.answer_seen = self.cur[1][0]\n",
        "        self.truth = \"\".join(self.entry['o'])\n",
        "\n",
        "        #################################################################\n",
        "        ## if do bert, we need to squeeze the space #####################\n",
        "        #################################################################\n",
        "        if do_bert:\n",
        "          self.truth = token_word(self.truth)\n",
        "        # get reference values\n",
        "        filename = \"%s.csv\" % self.entry['p']\n",
        "        related_triples_to_use = pd.read_csv(kg_dir + filename, sep='\\t', header = None)\n",
        "        self.reference_values = related_triples_to_use[2].values\n",
        "\n",
        "\n",
        "        #################################################################\n",
        "        ## Control KG usage #############################################\n",
        "        #################################################################\n",
        "        self.use_kg_index = self.entry['kg_index']\n",
        "\n",
        "\n",
        "    def __init__(self, env_config, T=20):\n",
        "      \n",
        "        \"\"\"\n",
        "        initialize the environment\n",
        "        \"\"\"\n",
        "        self.idx_to_test = env_config[\"idx_to_test\"]\n",
        "        self.training = env_config[\"training\"]\n",
        "        self.train_data = train_data\n",
        "        self.test_data = test_data\n",
        "        self._counter = 0 # For terminating the episode\n",
        "        self._build_init()\n",
        "        self.state = self.getState(self.cur, self.new)\n",
        "        self._horizon = env_config.get(\"T\", T)\n",
        "        self._setup_spaces()\n",
        "\n",
        "    def _setup_spaces(self):\n",
        "        ##############\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(-np.inf, np.inf, \n",
        "                                            [1 + 2 + 2 * len_similarity_features, ])\n",
        "        ##############\n",
        "\n",
        "    def step(self, action):\n",
        "        self.new_index += 1\n",
        "        if self.new_index >= self.max_index: # exceed the given size will stop (10 in paper)\n",
        "            reward = similarity_metrics.LevenSim(self.curans[0], self.truth)\n",
        "            done = True\n",
        "            return self.state, reward, done, {\"final_answer\":self.curans,\n",
        "                                              \"steps\": self.new_index}\n",
        "\n",
        "        else:\n",
        "          if action == 0: # still use old current as current\n",
        "              self.new = self.text_answer[self.new_index]\n",
        "              self.newans = self.new[1][0]\n",
        "              self.state = self.getState(self.cur, self.new)\n",
        "              reward = 0\n",
        "\n",
        "          elif action == 1: # accept new as current\n",
        "              self.cur_index = self.new_index - 1\n",
        "              self.cur = self.text_answer[self.cur_index]\n",
        "              self.curans = self.cur[1][0]\n",
        "              self.new = self.text_answer[self.new_index]\n",
        "              self.newans = self.new[1][0]\n",
        "              self.state = self.getState(self.cur, self.new)\n",
        "              reward = 0\n",
        "          else:\n",
        "              #reward = max([similarity_metrics.LevenSim(self.curans[i], self.truth) for i in range(self.K)])    #reward改成几种similarity的max\n",
        "              \n",
        "              reward = similarity_metrics.LevenSim(self.curans[0], self.truth)\n",
        "              done = True\n",
        "              return self.state, reward, done, {\"final_answer\":self.curans,\n",
        "                                              \"steps\": self.new_index}\n",
        "          self._counter += 1\n",
        "          done = self._counter >= self._horizon\n",
        "          return self.state, reward, done,  {\"final_answer\":self.curans,\n",
        "                                              \"steps\": self.new_index}\n",
        "\n",
        "    def reset(self):\n",
        "        self._build_init()\n",
        "        self.state = self.getState(self.cur, self.new)\n",
        "        self._counter = 0\n",
        "        return self.state\n",
        "\n",
        "    def getState(self, cur, new):\n",
        "        # input: current best text and answer, new text and answer that seen before\n",
        "        # output: state\n",
        "        curans = cur[1][0]\n",
        "        newans = new[1][0]\n",
        "        # state (1) confidence scores (dim: K*2)\n",
        "        curconf = cur[1][1]  \n",
        "        newconf = new[1][1]\n",
        "\n",
        "        # state (2) similarity between texts (dim: 1)\n",
        "        textsim = [self.textSimilarity(cur[1], new[1])]\n",
        "        try:\n",
        "          textsim = [textSimilarity(cur[0], new[0])]\n",
        "        except: \n",
        "          textsim = [0]\n",
        "\n",
        "\n",
        "        # state (3) \n",
        "        if self.use_kg_index == 1:\n",
        "          flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "          ref_score_cur = flatten([get_sim_features(i, self.reference_values, do_bert = do_bert) for i in curans])\n",
        "          ref_score_new = flatten([get_sim_features(i, self.reference_values, do_bert = do_bert) for i in newans])\n",
        "        \n",
        "        else:\n",
        "          ref_score_cur = ref_score_new = [0]*len_similarity_features\n",
        "        \n",
        "        state = curconf + newconf + ref_score_cur + ref_score_new + textsim\n",
        "        return state\n",
        "\n",
        "    # function to compute cosine similarity bewteen two texts\n",
        "    def textSimilarity(self, text1, text2):\n",
        "        corpus = [text1, text2]\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        try:\n",
        "          tfidf = vectorizer.fit_transform(corpus)\n",
        "          words = vectorizer.get_feature_names()\n",
        "          similarity_matrix = cosine_similarity(tfidf)\n",
        "          similarity = similarity_matrix[0][1]\n",
        "        except:\n",
        "          similarity = 0\n",
        "        return similarity\n",
        "\n",
        "len(KGRLMask({\"training\":True, \"idx_to_test\":0}).state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4zc5VJqSqcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9881744c-3f37-4413-b5dd-8adbff6bb74a"
      },
      "source": [
        " ray.init(num_gpus=1, log_to_driver=False, local_mode=True, ignore_reinit_error=True)\n",
        "ModelCatalog.register_custom_model(\"keras_q_model\", DQNModel)\n",
        "\n",
        "qTrainer = DQNTrainer(env=KGRLMask, config={# config to pass to env class\n",
        "    \"model\": {\n",
        "        \"custom_model\": \"keras_q_model\"\n",
        "    },\n",
        "    \"env_config\": {\"training\": True, \"idx_to_test\":None},\n",
        "    \"buffer_size\":100,\n",
        "    \"lr_schedule\": [[0, 0.05], [20, 0.01], [30, 0.005], [50, 0.001]],\n",
        "    \"train_batch_size\":100\n",
        "  })\n",
        "\n",
        "\n",
        "total_iteration = 20\n",
        "prev_time = time.time()\n",
        "for i in range(total_iteration):\n",
        "    print(\"iteration {};\".format(i), \\\n",
        "          \"%d sec/iteration;\" % (time.time()- prev_time), \\\n",
        "          \"%d min remaining\" % ((total_iteration - i)*(time.time()- prev_time)/60))\n",
        "    prev_time = time.time()\n",
        "    qTrainer.train()\n",
        "print(\"Training Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-31 07:40:05,218\tERROR worker.py:1432 -- Calling ray.init() again after it has already been called.\n",
            "2021-03-31 07:40:05,219\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
            "2021-03-31 07:40:07,092\tINFO rollout_worker.py:768 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f5b5374f410>}\n",
            "2021-03-31 07:40:07,093\tINFO rollout_worker.py:769 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f5b539cf490>}\n",
            "2021-03-31 07:40:07,094\tINFO rollout_worker.py:370 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f5b539cf710>}\n",
            "2021-03-31 07:40:07,099\tWARNING sync_replay_optimizer.py:105 -- buffer_size=100 < replay_starts=1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 0; 0 sec/iteration; 0 min remaining\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1; 21 sec/iteration; 6 min remaining\n",
            "iteration 2; 23 sec/iteration; 6 min remaining\n",
            "iteration 3; 24 sec/iteration; 6 min remaining\n",
            "iteration 4; 22 sec/iteration; 5 min remaining\n",
            "iteration 5; 20 sec/iteration; 5 min remaining\n",
            "iteration 6; 27 sec/iteration; 6 min remaining\n",
            "iteration 7; 21 sec/iteration; 4 min remaining\n",
            "iteration 8; 24 sec/iteration; 4 min remaining\n",
            "iteration 9; 23 sec/iteration; 4 min remaining\n",
            "iteration 10; 26 sec/iteration; 4 min remaining\n",
            "iteration 11; 26 sec/iteration; 3 min remaining\n",
            "iteration 12; 21 sec/iteration; 2 min remaining\n",
            "iteration 13; 31 sec/iteration; 3 min remaining\n",
            "iteration 14; 30 sec/iteration; 3 min remaining\n",
            "iteration 15; 21 sec/iteration; 1 min remaining\n",
            "iteration 16; 21 sec/iteration; 1 min remaining\n",
            "iteration 17; 35 sec/iteration; 1 min remaining\n",
            "iteration 18; 24 sec/iteration; 0 min remaining\n",
            "iteration 19; 30 sec/iteration; 0 min remaining\n",
            "Training Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot9P0N0GSrF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13c5f16-9ae4-42e2-b422-e9ccd8a9f8f1"
      },
      "source": [
        "def evaluation_q(test_data, pred_test, qTrainer):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    for i in range(len(test_data)):\n",
        "      env = KGRLEnv({\"training\": False, \"idx_to_test\":i, \"train_data\" : None, \"test_data\": test_data,\"pred_train\":  None, \"pred_test\" : pred_test, \"do_bert\" : do_bert})\n",
        "      state = env.state\n",
        "      done = False\n",
        "      while not done:\n",
        "          action = qTrainer.compute_action(state)\n",
        "          state, reward, done, results = env.step(action)\n",
        "      rewards.append(reward)\n",
        "      steps.append(results['steps'])\n",
        "    return rewards, steps\n",
        "    \n",
        "reward_list, step_list = evaluation_q(test_data, pred_test, qTrainer)\n",
        "avg_reward = np.mean(reward_list)\n",
        "avg_steps = np.mean(step_list)\n",
        "\n",
        "GPU_reward = np.mean(np.array(reward_list)[GPU_test_index])\n",
        "GPU_steps = np.mean(np.array(step_list)[GPU_test_index])\n",
        "\n",
        "Movie_reward = np.mean(np.array(reward_list)[Movie_test_index])\n",
        "Movie_steps = np.mean(np.array(step_list)[Movie_test_index])\n",
        "\n",
        "Game_reward = np.mean(np.array(reward_list)[Game_test_index])\n",
        "Game_steps = np.mean(np.array(step_list)[Game_test_index])\n",
        "\n",
        "Phone_reward = np.mean(np.array(reward_list)[Phone_test_index])\n",
        "Phone_steps = np.mean(np.array(step_list)[Phone_test_index])\n",
        "\n",
        "\n",
        "print(\"Training iteration {}..., \\n average reward is {:0.3f},\\\n",
        "average # of steps is {:0.3f}\".format(i, avg_reward, avg_steps))\n",
        "\n",
        "print(\"Average rewards for GPU/Movie/Game/Phone are {:0.3f}/{:0.3f}/{:0.3f}/{:0.3f}\"\\\n",
        "      .format(GPU_reward, Movie_reward, Game_reward, Phone_reward))\n",
        "\n",
        "print(\"Average # of steps for GPU/Movie/Game/Phone are {:0.3f}/{:0.3f}/{:0.3f}/{:0.3f}\"\\\n",
        "      .format(GPU_steps, Movie_steps, Game_steps, Phone_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training iteration 19..., \n",
            " average reward is 0.744,average # of steps is 3.823\n",
            "Average rewards for GPU/Movie/Game/Phone are 0.767/0.746/0.624/0.840\n",
            "Average # of steps for GPU/Movie/Game/Phone are 3.720/3.200/4.987/3.387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OSfShoQBowE"
      },
      "source": [
        "# Bidaf Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7VjTQnRIswx"
      },
      "source": [
        "method = \"bidaf\" ## possible methods: \"bert\", \"bidaf\", \"qanet\"\n",
        "########################################\n",
        "## read preprocessed data(train/test) ##\n",
        "########################################\n",
        "train_data = pickle.load(open(\"drive/MyDrive/preprocessed_data/train_data.pkl\", \"rb\" ))\n",
        "test_data = pickle.load(open(\"drive/MyDrive/preprocessed_data/test_data.pkl\", \"rb\" ))\n",
        "test_data_types = [test_data[i]['type'] for i in range(len(test_data))]\n",
        "\n",
        "do_bert = False\n",
        "if method == \"bert\":\n",
        "  do_bert = True\n",
        "#######################################################\n",
        "### Obtain the pickled predition for train and test ###\n",
        "#######################################################\n",
        "saving_path =\"drive/MyDrive/preprocessed_data/\"\n",
        "train_file = \"pred_train_%s.pkl\" %method ; test_file = \"pred_test_%s.pkl\" %method\n",
        "pred_train = pickle.load(open(saving_path + train_file, \"rb\" ))\n",
        "pred_test = pickle.load(open(saving_path + test_file, \"rb\" ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMHyypOiBuiS",
        "outputId": "c36cf4cb-b6da-4afc-caf8-2ef316cb64a5"
      },
      "source": [
        "print(\"+\" * 30 + \" For method %s, the scores are \" %method + \"+\" * 30)\n",
        "avg_score = []\n",
        "oracle_score = []\n",
        "max_conf_score = []\n",
        "majority_vote_score = []\n",
        "first_score = []\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "for i in range(len(test_data)):\n",
        "  cur = pred_test['Test_'+str(i)]\n",
        "  candidate_answer = [cur[j][0][0] for j in range(len(cur))]\n",
        "  confs = [cur[j][1][0] for j in range(len(cur))]\n",
        "  if do_bert: ## if do bert the object should be preprocessed as original bert paper\n",
        "    sims = [similarity_metrics.LevenSim(c, token_word(test_data[i]['o']) ) for c in candidate_answer]\n",
        "  else:\n",
        "    sims = [similarity_metrics.LevenSim(c, test_data[i]['o']) for c in candidate_answer]\n",
        "  # majority vote\n",
        "  c = Counter(candidate_answer)\n",
        "  ans_majority_vote, _ = c.most_common()[0]\n",
        "  first_score.append(sims[0])\n",
        "  avg_score.append(np.mean(sims))\n",
        "  max_conf_score.append(sims[np.argmax(confs)])\n",
        "  majority_vote_score.append(similarity_metrics.LevenSim(ans_majority_vote, \n",
        "                                                         test_data[i]['o']))\n",
        "  oracle_score.append(np.max(sims))\n",
        "\n",
        "test_data_types = [test_data[i]['type'] for i in range(len(test_data))]\n",
        "GPU_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'GPUs']\n",
        "Game_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'Games']\n",
        "Movie_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'Movies']\n",
        "Phone_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'phones']\n",
        "\n",
        "## print the scores \n",
        "\n",
        "print(\"first score is %0.3f (Overall)\"%np.mean(np.array(first_score)))\n",
        "print(\"avg score is %0.3f (Overall)\"%np.mean(np.array(avg_score)))\n",
        "print(\"mac conf score is %0.3f (Overall)\"%np.mean(np.array(max_conf_score)))\n",
        "print(\"majority vote score is %0.3f (Overall)\"%np.mean(np.array(majority_vote_score)))\n",
        "print(\"oracle score is %0.3f (Overall)\"%np.mean(np.array(oracle_score)))\n",
        "\n",
        "index_to_use = GPU_test_index\n",
        "print(\"first score is %0.3f for GPU\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for GPU\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for GPU\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for GPU\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for GPU\"%np.mean(np.array(oracle_score)[index_to_use]))\n",
        "\n",
        "index_to_use = Game_test_index\n",
        "print(\"first score is %0.3f for GAME\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for GAME\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for GAME\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for GAME\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for GAME\"%np.mean(np.array(oracle_score)[index_to_use]))\n",
        "\n",
        "index_to_use = Movie_test_index\n",
        "print(\"first score is %0.3f for MOVIE\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for MOVIE\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for MOVIE\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for MOVIE\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for MOVIE\"%np.mean(np.array(oracle_score)[index_to_use]))\n",
        "\n",
        "index_to_use = Phone_test_index\n",
        "print(\"first score is %0.3f for PHONE\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for PHONE\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for PHONE\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for PHONE\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for PHONE\"%np.mean(np.array(oracle_score)[index_to_use]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++++++++++++ For method bidaf, the scores are ++++++++++++++++++++++++++++++\n",
            "first score is 0.528 (Overall)\n",
            "avg score is 0.226 (Overall)\n",
            "mac conf score is 0.623 (Overall)\n",
            "majority vote score is 0.441 (Overall)\n",
            "oracle score is 0.838 (Overall)\n",
            "first score is 0.451 for GPU\n",
            "avg score is 0.259 for GPU\n",
            "mac conf score is 0.799 for GPU\n",
            "majority vote score is 0.488 for GPU\n",
            "oracle score is 0.902 for GPU\n",
            "first score is 0.498 for GAME\n",
            "avg score is 0.155 for GAME\n",
            "mac conf score is 0.488 for GAME\n",
            "majority vote score is 0.321 for GAME\n",
            "oracle score is 0.793 for GAME\n",
            "first score is 0.533 for MOVIE\n",
            "avg score is 0.267 for MOVIE\n",
            "mac conf score is 0.645 for MOVIE\n",
            "majority vote score is 0.539 for MOVIE\n",
            "oracle score is 0.846 for MOVIE\n",
            "first score is 0.632 for PHONE\n",
            "avg score is 0.223 for PHONE\n",
            "mac conf score is 0.560 for PHONE\n",
            "majority vote score is 0.415 for PHONE\n",
            "oracle score is 0.812 for PHONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlZ0KN3WB438",
        "outputId": "494a3b20-49e9-4533-900c-96fe3c7213a9"
      },
      "source": [
        "seed = 20201015\n",
        "np.random.seed(seed)\n",
        "tf.random.set_random_seed(seed)\n",
        "ray.init(num_gpus=1, log_to_driver=False, local_mode=True, ignore_reinit_error=True)\n",
        "ModelCatalog.register_custom_model(\"keras_q_model\", DQNModel)\n",
        "\n",
        "qTrainer = DQNTrainer(env=KGRLEnv, config={# config to pass to env class\n",
        "    \"model\": {\n",
        "        \"custom_model\": \"keras_q_model\"\n",
        "    },\n",
        "    \"seed\" : seed,\n",
        "    \"env_config\": {\"training\": True, \"idx_to_test\":None, \"train_data\" : train_data,\"test_data\": test_data,\"pred_train\":  pred_train, \"pred_test\" : pred_test, \"do_bert\" : do_bert},\n",
        "    \"buffer_size\":100,\n",
        "    \"lr_schedule\": [[0, 0.05], [20, 0.01], [30, 0.005], [50, 0.001]],\n",
        "    \"train_batch_size\":100\n",
        "  })\n",
        "\n",
        "\n",
        "total_iteration = 20\n",
        "prev_time = time.time()\n",
        "for i in range(total_iteration):\n",
        "    print(\"iteration {};\".format(i), \\\n",
        "          \"%d sec/iteration;\" % (time.time()- prev_time), \\\n",
        "          \"%d min remaining\" % ((total_iteration - i)*(time.time()- prev_time)/60))\n",
        "    prev_time = time.time()\n",
        "    qTrainer.train()\n",
        "    \n",
        "print(\"Training Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-24 02:21:04,698\tERROR worker.py:1432 -- Calling ray.init() again after it has already been called.\n",
            "2021-03-24 02:21:04,700\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
            "2021-03-24 02:21:06,877\tINFO rollout_worker.py:768 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f0e8951ee10>}\n",
            "2021-03-24 02:21:06,878\tINFO rollout_worker.py:769 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f0f20b3fe10>}\n",
            "2021-03-24 02:21:06,879\tINFO rollout_worker.py:370 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f0f20b3fb90>}\n",
            "2021-03-24 02:21:06,883\tWARNING sync_replay_optimizer.py:105 -- buffer_size=100 < replay_starts=1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 0; 0 sec/iteration; 0 min remaining\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1; 33 sec/iteration; 10 min remaining\n",
            "iteration 2; 37 sec/iteration; 11 min remaining\n",
            "iteration 3; 37 sec/iteration; 10 min remaining\n",
            "iteration 4; 36 sec/iteration; 9 min remaining\n",
            "iteration 5; 37 sec/iteration; 9 min remaining\n",
            "iteration 6; 37 sec/iteration; 8 min remaining\n",
            "iteration 7; 38 sec/iteration; 8 min remaining\n",
            "iteration 8; 37 sec/iteration; 7 min remaining\n",
            "iteration 9; 41 sec/iteration; 7 min remaining\n",
            "iteration 10; 38 sec/iteration; 6 min remaining\n",
            "iteration 11; 38 sec/iteration; 5 min remaining\n",
            "iteration 12; 36 sec/iteration; 4 min remaining\n",
            "iteration 13; 37 sec/iteration; 4 min remaining\n",
            "iteration 14; 38 sec/iteration; 3 min remaining\n",
            "iteration 15; 38 sec/iteration; 3 min remaining\n",
            "iteration 16; 36 sec/iteration; 2 min remaining\n",
            "iteration 17; 39 sec/iteration; 1 min remaining\n",
            "iteration 18; 39 sec/iteration; 1 min remaining\n",
            "iteration 19; 37 sec/iteration; 0 min remaining\n",
            "Training Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQCnzDzXCQs6",
        "outputId": "2de2b533-e527-4392-ae4d-10d6e3b77e61"
      },
      "source": [
        "def evaluation_q(test_data, pred_test, qTrainer):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    for i in range(len(test_data)):\n",
        "      env = KGRLEnv({\"training\": False, \"idx_to_test\":i, \"train_data\" : None, \"test_data\": test_data,\"pred_train\":  None, \"pred_test\" : pred_test, \"do_bert\" : do_bert})\n",
        "      state = env.state\n",
        "      done = False\n",
        "      while not done:\n",
        "          action = qTrainer.compute_action(state)\n",
        "          state, reward, done, results = env.step(action)\n",
        "      rewards.append(reward)\n",
        "      steps.append(results['steps'])\n",
        "    return rewards, steps\n",
        "\n",
        "reward_list, step_list = evaluation_q(test_data, pred_test, qTrainer)\n",
        "avg_reward = np.mean(reward_list)\n",
        "avg_steps = np.mean(step_list)\n",
        "\n",
        "GPU_reward = np.mean(np.array(reward_list)[GPU_test_index])\n",
        "GPU_steps = np.mean(np.array(step_list)[GPU_test_index])\n",
        "\n",
        "Movie_reward = np.mean(np.array(reward_list)[Movie_test_index])\n",
        "Movie_steps = np.mean(np.array(step_list)[Movie_test_index])\n",
        "\n",
        "Game_reward = np.mean(np.array(reward_list)[Game_test_index])\n",
        "Game_steps = np.mean(np.array(step_list)[Game_test_index])\n",
        "\n",
        "Phone_reward = np.mean(np.array(reward_list)[Phone_test_index])\n",
        "Phone_steps = np.mean(np.array(step_list)[Phone_test_index])\n",
        "print(\"Training iteration {}..., \\n average reward is {:0.3f},\\\n",
        "average # of steps is {:0.3f}\".format(i, avg_reward, avg_steps))\n",
        "\n",
        "print(\"Average rewards for GPU/Movie/Game/Phone are {:0.3f}/{:0.3f}/{:0.3f}/{:0.3f}\"\\\n",
        "      .format(GPU_reward, Movie_reward, Game_reward, Phone_reward))\n",
        "\n",
        "print(\"Average # of steps for GPU/Movie/Game/Phone are {:0.3f}/{:0.3f}/{:0.3f}/{:0.3f}\"\\\n",
        "      .format(GPU_steps, Movie_steps, Game_steps, Phone_steps))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training iteration 19..., \n",
            " average reward is 0.622,average # of steps is 8.710\n",
            "Average rewards for GPU/Movie/Game/Phone are 0.669/0.648/0.561/0.611\n",
            "Average # of steps for GPU/Movie/Game/Phone are 8.893/8.640/8.653/8.653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFzBtJv0T5Oc"
      },
      "source": [
        "use_percentage = 0.5 # control the percentage of data that can leverage KG\n",
        "\n",
        "train_kg_index = np.random.binomial(1, use_percentage, len(train_data))\n",
        "test_kg_index = np.random.binomial(1, use_percentage, len(test_data))\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "  train_data[i]['kg_index'] = train_kg_index[i]\n",
        "for i in range(len(test_data)):\n",
        "  test_data[i]['kg_index'] = test_kg_index[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2AwLhPIUAJv",
        "outputId": "ba5ff82a-f76e-4da5-fd3a-e9edd55a865e"
      },
      "source": [
        "class KGRLMask(gym.Env):\n",
        "    def _build_init(self, kg_dir = \"drive/MyDrive/related_triples_by_relation/\"):\n",
        "        \"\"\"\n",
        "        when testing, use idx_to_test, otherwise randomly sample a training data\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "          idx = np.random.choice(range(len(self.train_data))) \n",
        "          self.entry = self.train_data[idx]\n",
        "        else:\n",
        "          idx = self.idx_to_test\n",
        "          self.entry = self.test_data[idx]\n",
        "        self.query = self.entry['s'] + ' ' + self.entry['p']\n",
        "        self.text_list = self.entry['corpus']\n",
        "        ######################################################\n",
        "        ## obtain the answer from extraction system output ###\n",
        "        ######################################################\n",
        "        if self.training:\n",
        "          self.answer_list = pred_train[self.entry['id']] \n",
        "        else:\n",
        "          self.answer_list = pred_test[self.entry['id']]\n",
        "        self.text_answer = [[self.text_list[i], self.answer_list[i]] for i in range(len(self.text_list))]\n",
        "        self.max_index = len(self.text_list)\n",
        "        ### #####################################################################\n",
        "        ## initialize the index of current/new candidate as 0/1 respectively. ###\n",
        "        #########################################################################\n",
        "        self.cur_index = 0\n",
        "        self.new_index = 1\n",
        "        self.cur = self.text_answer[self.cur_index]\n",
        "        try:\n",
        "          self.new = self.text_answer[self.new_index]\n",
        "        except:\n",
        "          ####################################################################\n",
        "          ## exception would happen when size of raw text is less than 2. ####\n",
        "          ## which cannot happen in preprocessed data ########################\n",
        "          ####################################################################\n",
        "          self.new =  self.cur\n",
        "        self.curans = self.cur[1][0]\n",
        "        self.newans = self.new[1][0]\n",
        "        self.answer_seen = self.cur[1][0]\n",
        "        self.truth = \"\".join(self.entry['o'])\n",
        "\n",
        "        #################################################################\n",
        "        ## if do bert, we need to squeeze the space #####################\n",
        "        #################################################################\n",
        "        if do_bert:\n",
        "          self.truth = token_word(self.truth)\n",
        "        # get reference values\n",
        "        filename = \"%s.csv\" % self.entry['p']\n",
        "        related_triples_to_use = pd.read_csv(kg_dir + filename, sep='\\t', header = None)\n",
        "        self.reference_values = related_triples_to_use[2].values\n",
        "\n",
        "\n",
        "        #################################################################\n",
        "        ## Control KG usage #############################################\n",
        "        #################################################################\n",
        "        self.use_kg_index = self.entry['kg_index']\n",
        "\n",
        "\n",
        "    def __init__(self, env_config, T=20):\n",
        "      \n",
        "        \"\"\"\n",
        "        initialize the environment\n",
        "        \"\"\"\n",
        "        self.idx_to_test = env_config[\"idx_to_test\"]\n",
        "        self.training = env_config[\"training\"]\n",
        "        self.train_data = train_data\n",
        "        self.test_data = test_data\n",
        "        self._counter = 0 # For terminating the episode\n",
        "        self._build_init()\n",
        "        self.state = self.getState(self.cur, self.new)\n",
        "        self._horizon = env_config.get(\"T\", T)\n",
        "        self._setup_spaces()\n",
        "\n",
        "    def _setup_spaces(self):\n",
        "        ##############\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(-np.inf, np.inf, \n",
        "                                            [1 + 2 + 2 * len_similarity_features, ])\n",
        "        ##############\n",
        "\n",
        "    def step(self, action):\n",
        "        self.new_index += 1\n",
        "        if self.new_index >= self.max_index: # exceed the given size will stop (10 in paper)\n",
        "            reward = similarity_metrics.LevenSim(self.curans[0], self.truth)\n",
        "            done = True\n",
        "            return self.state, reward, done, {\"final_answer\":self.curans,\n",
        "                                              \"steps\": self.new_index}\n",
        "\n",
        "        else:\n",
        "          if action == 0: # still use old current as current\n",
        "              self.new = self.text_answer[self.new_index]\n",
        "              self.newans = self.new[1][0]\n",
        "              self.state = self.getState(self.cur, self.new)\n",
        "              reward = 0\n",
        "\n",
        "          elif action == 1: # accept new as current\n",
        "              self.cur_index = self.new_index - 1\n",
        "              self.cur = self.text_answer[self.cur_index]\n",
        "              self.curans = self.cur[1][0]\n",
        "              self.new = self.text_answer[self.new_index]\n",
        "              self.newans = self.new[1][0]\n",
        "              self.state = self.getState(self.cur, self.new)\n",
        "              reward = 0\n",
        "          else:\n",
        "              #reward = max([similarity_metrics.LevenSim(self.curans[i], self.truth) for i in range(self.K)])    #reward改成几种similarity的max\n",
        "              \n",
        "              reward = similarity_metrics.LevenSim(self.curans[0], self.truth)\n",
        "              done = True\n",
        "              return self.state, reward, done, {\"final_answer\":self.curans,\n",
        "                                              \"steps\": self.new_index}\n",
        "          self._counter += 1\n",
        "          done = self._counter >= self._horizon\n",
        "          return self.state, reward, done,  {\"final_answer\":self.curans,\n",
        "                                              \"steps\": self.new_index}\n",
        "\n",
        "    def reset(self):\n",
        "        self._build_init()\n",
        "        self.state = self.getState(self.cur, self.new)\n",
        "        self._counter = 0\n",
        "        return self.state\n",
        "\n",
        "    def getState(self, cur, new):\n",
        "        # input: current best text and answer, new text and answer that seen before\n",
        "        # output: state\n",
        "        curans = cur[1][0]\n",
        "        newans = new[1][0]\n",
        "        # state (1) confidence scores (dim: K*2)\n",
        "        curconf = cur[1][1]  \n",
        "        newconf = new[1][1]\n",
        "\n",
        "        # state (2) similarity between texts (dim: 1)\n",
        "        textsim = [self.textSimilarity(cur[1], new[1])]\n",
        "        try:\n",
        "          textsim = [textSimilarity(cur[0], new[0])]\n",
        "        except: \n",
        "          textsim = [0]\n",
        "\n",
        "\n",
        "        # state (3) \n",
        "        if self.use_kg_index == 1:\n",
        "          flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "          ref_score_cur = flatten([get_sim_features(i, self.reference_values, do_bert = do_bert) for i in curans])\n",
        "          ref_score_new = flatten([get_sim_features(i, self.reference_values, do_bert = do_bert) for i in newans])\n",
        "        \n",
        "        else:\n",
        "          ref_score_cur = ref_score_new = [0]*len_similarity_features\n",
        "        \n",
        "        state = curconf + newconf + ref_score_cur + ref_score_new + textsim\n",
        "        return state\n",
        "\n",
        "    # function to compute cosine similarity bewteen two texts\n",
        "    def textSimilarity(self, text1, text2):\n",
        "        corpus = [text1, text2]\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        try:\n",
        "          tfidf = vectorizer.fit_transform(corpus)\n",
        "          words = vectorizer.get_feature_names()\n",
        "          similarity_matrix = cosine_similarity(tfidf)\n",
        "          similarity = similarity_matrix[0][1]\n",
        "        except:\n",
        "          similarity = 0\n",
        "        return similarity\n",
        "\n",
        "len(KGRLMask({\"training\":True, \"idx_to_test\":0}).state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBAot2LVUJ45",
        "outputId": "dfdb9e58-6c54-45de-c50a-8f2fc5ff358b"
      },
      "source": [
        "ray.init(num_gpus=1, log_to_driver=False, local_mode=True, ignore_reinit_error=True)\n",
        "ModelCatalog.register_custom_model(\"keras_q_model\", DQNModel)\n",
        "\n",
        "qTrainer = DQNTrainer(env=KGRLMask, config={# config to pass to env class\n",
        "    \"model\": {\n",
        "        \"custom_model\": \"keras_q_model\"\n",
        "    },\n",
        "    \"env_config\": {\"training\": True, \"idx_to_test\":None},\n",
        "    \"buffer_size\":100,\n",
        "    \"lr_schedule\": [[0, 0.05], [20, 0.01], [30, 0.005], [50, 0.001]],\n",
        "    \"train_batch_size\":100\n",
        "  })\n",
        "\n",
        "\n",
        "total_iteration = 20\n",
        "prev_time = time.time()\n",
        "for i in range(total_iteration):\n",
        "    print(\"iteration {};\".format(i), \\\n",
        "          \"%d sec/iteration;\" % (time.time()- prev_time), \\\n",
        "          \"%d min remaining\" % ((total_iteration - i)*(time.time()- prev_time)/60))\n",
        "    prev_time = time.time()\n",
        "    qTrainer.train()\n",
        "print(\"Training Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-24 03:39:40,252\tERROR worker.py:1432 -- Calling ray.init() again after it has already been called.\n",
            "2021-03-24 03:39:40,254\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
            "2021-03-24 03:39:42,077\tINFO rollout_worker.py:768 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f0e8b7b2b10>}\n",
            "2021-03-24 03:39:42,078\tINFO rollout_worker.py:769 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f0e8b393750>}\n",
            "2021-03-24 03:39:42,079\tINFO rollout_worker.py:370 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f0e8951ed10>}\n",
            "2021-03-24 03:39:42,082\tWARNING sync_replay_optimizer.py:105 -- buffer_size=100 < replay_starts=1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 0; 0 sec/iteration; 0 min remaining\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1; 19 sec/iteration; 6 min remaining\n",
            "iteration 2; 22 sec/iteration; 6 min remaining\n",
            "iteration 3; 20 sec/iteration; 5 min remaining\n",
            "iteration 4; 19 sec/iteration; 5 min remaining\n",
            "iteration 5; 20 sec/iteration; 5 min remaining\n",
            "iteration 6; 22 sec/iteration; 5 min remaining\n",
            "iteration 7; 23 sec/iteration; 5 min remaining\n",
            "iteration 8; 20 sec/iteration; 4 min remaining\n",
            "iteration 9; 15 sec/iteration; 2 min remaining\n",
            "iteration 10; 15 sec/iteration; 2 min remaining\n",
            "iteration 11; 21 sec/iteration; 3 min remaining\n",
            "iteration 12; 20 sec/iteration; 2 min remaining\n",
            "iteration 13; 29 sec/iteration; 3 min remaining\n",
            "iteration 14; 26 sec/iteration; 2 min remaining\n",
            "iteration 15; 16 sec/iteration; 1 min remaining\n",
            "iteration 16; 29 sec/iteration; 1 min remaining\n",
            "iteration 17; 25 sec/iteration; 1 min remaining\n",
            "iteration 18; 25 sec/iteration; 0 min remaining\n",
            "iteration 19; 32 sec/iteration; 0 min remaining\n",
            "Training Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkAr2gjzhcw7"
      },
      "source": [
        "# QANET method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhUmwg8-UP9P"
      },
      "source": [
        "method = \"qanet\" ## possible methods: \"bert\", \"bidaf\", \"qanet\"\n",
        "########################################\n",
        "## read preprocessed data(train/test) ##\n",
        "########################################\n",
        "train_data = pickle.load(open(\"drive/MyDrive/preprocessed_data/train_data.pkl\", \"rb\" ))\n",
        "test_data = pickle.load(open(\"drive/MyDrive/preprocessed_data/test_data.pkl\", \"rb\" ))\n",
        "test_data_types = [test_data[i]['type'] for i in range(len(test_data))]\n",
        "\n",
        "do_bert = False\n",
        "if method == \"bert\":\n",
        "  do_bert = True\n",
        "#######################################################\n",
        "### Obtain the pickled predition for train and test ###\n",
        "#######################################################\n",
        "saving_path =\"drive/MyDrive/preprocessed_data/\"\n",
        "train_file = \"pred_train_%s.pkl\" %method ; test_file = \"pred_test_%s.pkl\" %method\n",
        "pred_train = pickle.load(open(saving_path + train_file, \"rb\" ))\n",
        "pred_test = pickle.load(open(saving_path + test_file, \"rb\" ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz-eiZUghld5",
        "outputId": "dd3f7ade-9977-4366-dfd0-268ecc72d20e"
      },
      "source": [
        "print(\"+\" * 30 + \" For method %s, the scores are \" %method + \"+\" * 30)\n",
        "avg_score = []\n",
        "oracle_score = []\n",
        "max_conf_score = []\n",
        "majority_vote_score = []\n",
        "first_score = []\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "for i in range(len(test_data)):\n",
        "  cur = pred_test['Test_'+str(i)]\n",
        "  candidate_answer = [cur[j][0][0] for j in range(len(cur))]\n",
        "  confs = [cur[j][1][0] for j in range(len(cur))]\n",
        "  if do_bert: ## if do bert the object should be preprocessed as original bert paper\n",
        "    sims = [similarity_metrics.LevenSim(c, token_word(test_data[i]['o']) ) for c in candidate_answer]\n",
        "  else:\n",
        "    sims = [similarity_metrics.LevenSim(c, test_data[i]['o']) for c in candidate_answer]\n",
        "  # majority vote\n",
        "  c = Counter(candidate_answer)\n",
        "  ans_majority_vote, _ = c.most_common()[0]\n",
        "  first_score.append(sims[0])\n",
        "  avg_score.append(np.mean(sims))\n",
        "  max_conf_score.append(sims[np.argmax(confs)])\n",
        "  majority_vote_score.append(similarity_metrics.LevenSim(ans_majority_vote, \n",
        "                                                         test_data[i]['o']))\n",
        "  oracle_score.append(np.max(sims))\n",
        "\n",
        "test_data_types = [test_data[i]['type'] for i in range(len(test_data))]\n",
        "GPU_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'GPUs']\n",
        "Game_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'Games']\n",
        "Movie_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'Movies']\n",
        "Phone_test_index = [i for i in range(len(test_data)) if test_data_types[i] == 'phones']\n",
        "\n",
        "## print the scores \n",
        "\n",
        "print(\"first score is %0.3f (Overall)\"%np.mean(np.array(first_score)))\n",
        "print(\"avg score is %0.3f (Overall)\"%np.mean(np.array(avg_score)))\n",
        "print(\"mac conf score is %0.3f (Overall)\"%np.mean(np.array(max_conf_score)))\n",
        "print(\"majority vote score is %0.3f (Overall)\"%np.mean(np.array(majority_vote_score)))\n",
        "print(\"oracle score is %0.3f (Overall)\"%np.mean(np.array(oracle_score)))\n",
        "\n",
        "index_to_use = GPU_test_index\n",
        "print(\"first score is %0.3f for GPU\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for GPU\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for GPU\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for GPU\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for GPU\"%np.mean(np.array(oracle_score)[index_to_use]))\n",
        "\n",
        "index_to_use = Game_test_index\n",
        "print(\"first score is %0.3f for GAME\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for GAME\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for GAME\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for GAME\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for GAME\"%np.mean(np.array(oracle_score)[index_to_use]))\n",
        "\n",
        "index_to_use = Movie_test_index\n",
        "print(\"first score is %0.3f for MOVIE\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for MOVIE\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for MOVIE\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for MOVIE\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for MOVIE\"%np.mean(np.array(oracle_score)[index_to_use]))\n",
        "\n",
        "index_to_use = Phone_test_index\n",
        "print(\"first score is %0.3f for PHONE\"%np.mean(np.array(first_score)[index_to_use]))\n",
        "print(\"avg score is %0.3f for PHONE\"%np.mean(np.array(avg_score)[index_to_use]))\n",
        "print(\"mac conf score is %0.3f for PHONE\"%np.mean(np.array(max_conf_score)[index_to_use]))\n",
        "print(\"majority vote score is %0.3f for PHONE\"%np.mean(np.array(majority_vote_score)[index_to_use]))\n",
        "print(\"oracle score is %0.3f for PHONE\"%np.mean(np.array(oracle_score)[index_to_use]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++++++++++++ For method qanet, the scores are ++++++++++++++++++++++++++++++\n",
            "first score is 0.561 (Overall)\n",
            "avg score is 0.230 (Overall)\n",
            "mac conf score is 0.605 (Overall)\n",
            "majority vote score is 0.444 (Overall)\n",
            "oracle score is 0.880 (Overall)\n",
            "first score is 0.507 for GPU\n",
            "avg score is 0.261 for GPU\n",
            "mac conf score is 0.691 for GPU\n",
            "majority vote score is 0.484 for GPU\n",
            "oracle score is 0.932 for GPU\n",
            "first score is 0.533 for GAME\n",
            "avg score is 0.167 for GAME\n",
            "mac conf score is 0.493 for GAME\n",
            "majority vote score is 0.325 for GAME\n",
            "oracle score is 0.840 for GAME\n",
            "first score is 0.531 for MOVIE\n",
            "avg score is 0.259 for MOVIE\n",
            "mac conf score is 0.689 for MOVIE\n",
            "majority vote score is 0.500 for MOVIE\n",
            "oracle score is 0.878 for MOVIE\n",
            "first score is 0.675 for PHONE\n",
            "avg score is 0.236 for PHONE\n",
            "mac conf score is 0.546 for PHONE\n",
            "majority vote score is 0.469 for PHONE\n",
            "oracle score is 0.868 for PHONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS9rNQifhsZQ",
        "outputId": "93014481-b7b2-46a9-9c7f-23322e5fd280"
      },
      "source": [
        "seed = 20201015\n",
        "np.random.seed(seed)\n",
        "tf.random.set_random_seed(seed)\n",
        "ray.init(num_gpus=1, log_to_driver=False, local_mode=True, ignore_reinit_error=True)\n",
        "ModelCatalog.register_custom_model(\"keras_q_model\", DQNModel)\n",
        "\n",
        "qTrainer = DQNTrainer(env=KGRLEnv, config={# config to pass to env class\n",
        "    \"model\": {\n",
        "        \"custom_model\": \"keras_q_model\"\n",
        "    },\n",
        "    \"seed\" : seed,\n",
        "    \"env_config\": {\"training\": True, \"idx_to_test\":None, \"train_data\" : train_data,\"test_data\": test_data,\"pred_train\":  pred_train, \"pred_test\" : pred_test, \"do_bert\" : do_bert},\n",
        "    \"buffer_size\":100,\n",
        "    \"lr_schedule\": [[0, 0.05], [20, 0.01], [30, 0.005], [50, 0.001]],\n",
        "    \"train_batch_size\":100\n",
        "  })\n",
        "\n",
        "\n",
        "total_iteration = 20\n",
        "prev_time = time.time()\n",
        "for i in range(total_iteration):\n",
        "    print(\"iteration {};\".format(i), \\\n",
        "          \"%d sec/iteration;\" % (time.time()- prev_time), \\\n",
        "          \"%d min remaining\" % ((total_iteration - i)*(time.time()- prev_time)/60))\n",
        "    prev_time = time.time()\n",
        "    qTrainer.train()\n",
        "    \n",
        "print(\"Training Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-24 04:50:21,161\tERROR worker.py:1432 -- Calling ray.init() again after it has already been called.\n",
            "2021-03-24 04:50:21,163\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
            "2021-03-24 04:50:24,007\tINFO rollout_worker.py:768 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f0f1fe2c410>}\n",
            "2021-03-24 04:50:24,008\tINFO rollout_worker.py:769 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f0f20ac6f90>}\n",
            "2021-03-24 04:50:24,008\tINFO rollout_worker.py:370 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f0f1fd06b10>}\n",
            "2021-03-24 04:50:24,011\tWARNING sync_replay_optimizer.py:105 -- buffer_size=100 < replay_starts=1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 0; 0 sec/iteration; 0 min remaining\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1; 33 sec/iteration; 10 min remaining\n",
            "iteration 2; 39 sec/iteration; 11 min remaining\n",
            "iteration 3; 36 sec/iteration; 10 min remaining\n",
            "iteration 4; 36 sec/iteration; 9 min remaining\n",
            "iteration 5; 39 sec/iteration; 9 min remaining\n",
            "iteration 6; 39 sec/iteration; 9 min remaining\n",
            "iteration 7; 38 sec/iteration; 8 min remaining\n",
            "iteration 8; 36 sec/iteration; 7 min remaining\n",
            "iteration 9; 38 sec/iteration; 7 min remaining\n",
            "iteration 10; 39 sec/iteration; 6 min remaining\n",
            "iteration 11; 37 sec/iteration; 5 min remaining\n",
            "iteration 12; 39 sec/iteration; 5 min remaining\n",
            "iteration 13; 37 sec/iteration; 4 min remaining\n",
            "iteration 14; 36 sec/iteration; 3 min remaining\n",
            "iteration 15; 38 sec/iteration; 3 min remaining\n",
            "iteration 16; 37 sec/iteration; 2 min remaining\n",
            "iteration 17; 39 sec/iteration; 1 min remaining\n",
            "iteration 18; 37 sec/iteration; 1 min remaining\n",
            "iteration 19; 39 sec/iteration; 0 min remaining\n",
            "Training Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy9ZfonYkbUj",
        "outputId": "ca76ffe3-14fe-4a26-a7ff-8a3734e9ec65"
      },
      "source": [
        "def evaluation_q(test_data, pred_test, qTrainer):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    for i in range(len(test_data)):\n",
        "      env = KGRLEnv({\"training\": False, \"idx_to_test\":i, \"train_data\" : None, \"test_data\": test_data,\"pred_train\":  None, \"pred_test\" : pred_test, \"do_bert\" : do_bert})\n",
        "      state = env.state\n",
        "      done = False\n",
        "      while not done:\n",
        "          action = qTrainer.compute_action(state)\n",
        "          state, reward, done, results = env.step(action)\n",
        "      rewards.append(reward)\n",
        "      steps.append(results['steps'])\n",
        "    return rewards, steps\n",
        "\n",
        "reward_list, step_list = evaluation_q(test_data, pred_test, qTrainer)\n",
        "avg_reward = np.mean(reward_list)\n",
        "avg_steps = np.mean(step_list)\n",
        "\n",
        "GPU_reward = np.mean(np.array(reward_list)[GPU_test_index])\n",
        "GPU_steps = np.mean(np.array(step_list)[GPU_test_index])\n",
        "\n",
        "Movie_reward = np.mean(np.array(reward_list)[Movie_test_index])\n",
        "Movie_steps = np.mean(np.array(step_list)[Movie_test_index])\n",
        "\n",
        "Game_reward = np.mean(np.array(reward_list)[Game_test_index])\n",
        "Game_steps = np.mean(np.array(step_list)[Game_test_index])\n",
        "\n",
        "Phone_reward = np.mean(np.array(reward_list)[Phone_test_index])\n",
        "Phone_steps = np.mean(np.array(step_list)[Phone_test_index])\n",
        "print(\"Training iteration {}..., \\n average reward is {:0.3f},\\\n",
        "average # of steps is {:0.3f}\".format(i, avg_reward, avg_steps))\n",
        "\n",
        "print(\"Average rewards for GPU/Movie/Game/Phone are {:0.3f}/{:0.3f}/{:0.3f}/{:0.3f}\"\\\n",
        "      .format(GPU_reward, Movie_reward, Game_reward, Phone_reward))\n",
        "\n",
        "print(\"Average # of steps for GPU/Movie/Game/Phone are {:0.3f}/{:0.3f}/{:0.3f}/{:0.3f}\"\\\n",
        "      .format(GPU_steps, Movie_steps, Game_steps, Phone_steps))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training iteration 19..., \n",
            " average reward is 0.704,average # of steps is 5.403\n",
            "Average rewards for GPU/Movie/Game/Phone are 0.699/0.730/0.673/0.712\n",
            "Average # of steps for GPU/Movie/Game/Phone are 5.893/4.520/5.653/5.547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rVcwxs1r3Rl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}