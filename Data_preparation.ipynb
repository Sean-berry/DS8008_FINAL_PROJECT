{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "O04pkr6Z0j4G",
    "outputId": "c6c1fa19-7cff-4c72-8fe9-75dad53281b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Intentionally crashing session to use the newly installed library.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping pyarrow as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "print('NOTE: Intentionally crashing session to use the newly installed library.\\n')\n",
    "!pip uninstall -y pyarrow\n",
    "!pip install ray[debug]==0.7.5\n",
    "# A hack to force the runtime to restart, needed to include the above dependencies.\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "68wgJkDehIXJ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.colab\n",
      "  Using cached google_colab-1.0.0-py2.py3-none-any.whl\n",
      "Collecting portpicker~=1.2.0\n",
      "  Using cached portpicker-1.2.0-py3-none-any.whl\n",
      "Collecting ipython~=5.5.0\n",
      "  Using cached ipython-5.5.0-py3-none-any.whl (758 kB)\n",
      "Requirement already satisfied: six~=1.12.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from google.colab) (1.12.0)\n",
      "Collecting ipykernel~=4.6.0\n",
      "  Using cached ipykernel-4.6.1-py3-none-any.whl (104 kB)\n",
      "Collecting notebook~=5.2.0\n",
      "  Using cached notebook-5.2.2-py2.py3-none-any.whl (8.0 MB)\n",
      "Collecting requests~=2.21.0\n",
      "  Using cached requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
      "Collecting pandas~=0.24.0\n",
      "  Using cached pandas-0.24.2-cp38-cp38-win_amd64.whl\n",
      "Collecting google-auth~=1.4.0\n",
      "  Using cached google_auth-1.4.2-py2.py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: tornado~=4.5.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from google.colab) (4.5.3)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from google-auth~=1.4.0->google.colab) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from google-auth~=1.4.0->google.colab) (0.2.8)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from google-auth~=1.4.0->google.colab) (4.1.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\sean\\anaconda3\\lib\\site-packages (from ipykernel~=4.6.0->google.colab) (6.1.7)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from ipykernel~=4.6.0->google.colab) (5.0.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\sean\\anaconda3\\lib\\site-packages (from ipython~=5.5.0->google.colab) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from ipython~=5.5.0->google.colab) (50.3.2)\n",
      "Collecting simplegeneric>0.8\n",
      "  Using cached simplegeneric-0.8.1-py3-none-any.whl\n",
      "Requirement already satisfied: decorator in c:\\users\\sean\\anaconda3\\lib\\site-packages (from ipython~=5.5.0->google.colab) (4.4.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\sean\\anaconda3\\lib\\site-packages (from ipython~=5.5.0->google.colab) (2.7.2)\n",
      "Collecting prompt-toolkit<2.0.0,>=1.0.4\n",
      "  Using cached prompt_toolkit-1.0.18-py3-none-any.whl (245 kB)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\sean\\anaconda3\\lib\\site-packages (from ipython~=5.5.0->google.colab) (0.7.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from notebook~=5.2.0->google.colab) (2.11.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\sean\\anaconda3\\lib\\site-packages (from notebook~=5.2.0->google.colab) (4.6.3)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\sean\\anaconda3\\lib\\site-packages (from notebook~=5.2.0->google.colab) (0.2.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\sean\\anaconda3\\lib\\site-packages (from notebook~=5.2.0->google.colab) (5.0.8)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\sean\\anaconda3\\lib\\site-packages (from notebook~=5.2.0->google.colab) (6.0.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from pandas~=0.24.0->google.colab) (1.18.4)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\sean\\anaconda3\\lib\\site-packages (from pandas~=0.24.0->google.colab) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from pandas~=0.24.0->google.colab) (2.8.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sean\\anaconda3\\lib\\site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython~=5.5.0->google.colab) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=1.4.0->google.colab) (0.4.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from requests~=2.21.0->google.colab) (3.0.4)\n",
      "Collecting urllib3<1.25,>=1.21.1\n",
      "  Using cached urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from requests~=2.21.0->google.colab) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from requests~=2.21.0->google.colab) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from jinja2->notebook~=5.2.0->google.colab) (1.1.1)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel~=4.6.0->google.colab) (20.0.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from jupyter-core->notebook~=5.2.0->google.colab) (227)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.6.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbconvert->notebook~=5.2.0->google.colab) (1.4.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.5.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.8.4)\n",
      "Requirement already satisfied: testpath in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.4.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbconvert->notebook~=5.2.0->google.colab) (3.2.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook~=5.2.0->google.colab) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook~=5.2.0->google.colab) (1.4.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from nbformat->notebook~=5.2.0->google.colab) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (20.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\sean\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook~=5.2.0->google.colab) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\sean\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook~=5.2.0->google.colab) (20.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\sean\\anaconda3\\lib\\site-packages (from packaging->bleach->nbconvert->notebook~=5.2.0->google.colab) (2.4.7)\n",
      "Installing collected packages: simplegeneric, prompt-toolkit, ipython, urllib3, ipykernel, requests, portpicker, pandas, notebook, google-auth, google.colab\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.8\n",
      "    Uninstalling prompt-toolkit-3.0.8:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.8\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 7.19.0\n",
      "    Uninstalling ipython-7.19.0:\n",
      "      Successfully uninstalled ipython-7.19.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.3\n",
      "    Uninstalling urllib3-1.26.3:\n",
      "      Successfully uninstalled urllib3-1.26.3\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 5.3.4\n",
      "    Uninstalling ipykernel-5.3.4:\n",
      "      Successfully uninstalled ipykernel-5.3.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.0\n",
      "    Uninstalling requests-2.25.0:\n",
      "      Successfully uninstalled requests-2.25.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.3\n",
      "    Uninstalling pandas-1.0.3:\n",
      "      Successfully uninstalled pandas-1.0.3\n",
      "  Attempting uninstall: notebook\n",
      "    Found existing installation: notebook 6.1.5\n",
      "    Uninstalling notebook-6.1.5:\n",
      "      Successfully uninstalled notebook-6.1.5\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.23.0\n",
      "    Uninstalling google-auth-1.23.0:\n",
      "      Successfully uninstalled google-auth-1.23.0\n",
      "Successfully installed google-auth-1.4.2 google.colab ipykernel-4.6.1 ipython-5.5.0 notebook-6.1.4 pandas-1.0.1 portpicker-1.2.0 prompt-toolkit-1.0.18 requests-2.21.0 simplegeneric-0.8.1 urllib3-1.24.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.4.0 requires google-auth<2,>=1.6.3, but you have google-auth 1.4.2 which is incompatible.\n",
      "mxnet 1.6.0 requires numpy<1.17.0,>=1.8.2, but you have numpy 1.18.4 which is incompatible.\n",
      "mxnet 1.6.0 requires requests<2.19.0,>=2.18.4, but you have requests 2.21.0 which is incompatible.\n",
      "ipdb 0.13.6 requires ipython>=7.17.0; python_version > \"3.6\", but you have ipython 5.5.0 which is incompatible.\n",
      "google-api-core 1.26.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 1.4.2 which is incompatible.\n",
      "google-api-core 1.26.1 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "gluonts 0.6.0 requires pandas>=1.0, but you have pandas 0.24.2 which is incompatible.\n",
      "gluonts 0.6.0 requires pydantic<1.7,~=1.1, but you have pydantic 1.7.3 which is incompatible.\n",
      "botocore 1.20.24 requires urllib3<1.27,>=1.25.4, but you have urllib3 1.24.3 which is incompatible.\n",
      "allennlp 1.5.0 requires torch<1.8.0,>=1.6.0, but you have torch 1.5.0+cpu which is incompatible.\n",
      "allennlp 1.5.0 requires transformers<4.3,>=4.1, but you have transformers 3.1.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install google.colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xzq9OclK0pg5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install python-Levenshtein\n",
    "!pip install redis\n",
    "!pip install -U ray\n",
    "!pip install ray[debug]==0.7.5\n",
    "!pip install ray[rllib]  # also recommended: ray[debug]\n",
    "!pip uninstall -y pyarrow\n",
    "!pip install unicodedata2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSDz6m3g55L9"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9vknvL5gqi3X"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/content/drive/My Drive/Knowledge Extraction/szhang37_code/KG_RL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5fdc3eeedab0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/content/drive/My Drive/Knowledge Extraction/szhang37_code/KG_RL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/drive/My Drive/Knowledge Extraction/szhang37_code/KG_RL'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/Knowledge Extraction/szhang37_code/KG_RL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LedW5b8H0mki"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1vSh2nH0rST"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "#### Import ray related package\n",
    "import ray\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n",
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.agents.dqn.distributional_q_model import DistributionalQModel\n",
    "from ray.rllib.models.tf.visionnet_v2 import VisionNetwork as MyVisionNetwork\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.tune import grid_search\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune import run as run_tune\n",
    "from ray.tune.registry import register_env\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn import DQNTrainer, DEFAULT_CONFIG\n",
    "### import self-defined function\n",
    "import similarity_metrics\n",
    "from utility_function import *\n",
    "### import Environment\n",
    "from Environment import KGRLEnv\n",
    "### Import Customized DQN\n",
    "from PolicyDQN import *\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V23fjcNy0Tpm"
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "This section is used to obtain predicted data.\n",
    "\n",
    "Skip this section when saved predicted data is provided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-hLZkZKsYI-"
   },
   "source": [
    "### Bert\n",
    "\n",
    "distance training for bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w_rxgeNrx32"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "########################################################\n",
    "## convert the distance data into squad-like dataset ###\n",
    "########################################################\n",
    "\n",
    "def convert_distance_datasquad(file_name_input = 'sougou.json', file_name_output = None):\n",
    "  \n",
    "  data = {}\n",
    "  data['data'] = []\n",
    "  f = open(file_name_input, \"r\")\n",
    "  lines = f.readlines()\n",
    "  for i in range(len(lines)):\n",
    "    line = lines[i]\n",
    "    oneline = json.loads(line)\n",
    "    answer = oneline['answer']\n",
    "    passages = oneline['passages']\n",
    "    question = oneline['question']\n",
    "    for j in range(len(passages)):\n",
    "      start_id = passages[j]['passage_text'].find(answer)\n",
    "      if start_id == -1: continue\n",
    "      entry = {}\n",
    "      entry[\"paragraphs\"] = [{\n",
    "        'context' : passages[j]['passage_text'],\n",
    "        'qas' : [{'question' : question,\n",
    "                  'id' : \"Trainbaidu_\" + str(i) + \"_context_\" + str(j),\n",
    "                  'answers' : [{'answer_start': start_id, \n",
    "                                'text': answer}]  }]\n",
    "      }]\n",
    "      data['data'].append(entry)\n",
    "      break \n",
    "\n",
    "  with open(file_name_output, 'w') as outfile:\n",
    "      json.dump(data, outfile)\n",
    "  return data\n",
    "\n",
    "distance_data_dir = \"./distance_data/\"\n",
    "data = convert_distance_datasquad(file_name_input = distance_data_dir + 'sougou.json', file_name_output = distance_data_dir + 'sougou_squad.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCBBV6QZt4qu"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "## covert our data into squad-like ##\n",
    "#####################################\n",
    "\n",
    "def convert_squad(test_data, file_name = 'test_baidu_good.json'):\n",
    "  \n",
    "\n",
    "  data = {}\n",
    "  data['data'] = []\n",
    "  for i in range(len(test_data)):\n",
    "    answer = test_data[i]['o']\n",
    "    passages = test_data[i]['corpus']\n",
    "    question = test_data[i]['s'] + \"的\" + test_data[i]['p'] + \"是什么？\"\n",
    "    for j in range(len(passages)):\n",
    "      start_id = None\n",
    "      entry = {}\n",
    "      entry[\"paragraphs\"] = [{\n",
    "        'context' : passages[j],\n",
    "        'qas' : [{'question' : question,\n",
    "                  'id' : \"Trainbaidu_\" + str(i) + \"_context_\" + str(j),\n",
    "                  'answers' : [{'answer_start': start_id, \n",
    "                                'text': answer}]  }]\n",
    "      }]\n",
    "      data['data'].append(entry)\n",
    "      #print(i)\n",
    "\n",
    "        #break ## 每个问题有一个文本就好了。 (不然太太太多了？)\n",
    "\n",
    "\n",
    "  with open(file_name, 'w') as outfile:\n",
    "      json.dump(data, outfile)\n",
    "\n",
    "train_data = pickle.load(open(\"./preprocessed_data/train_data.pkl\", \"rb\" ))\n",
    "test_data = pickle.load(open(\"./preprocessed_data/test_data.pkl\", \"rb\" ))\n",
    "store_dir = \"./preprocessed_data/\"\n",
    "convert_squad(test_data, file_name = store_dir + 'test_data_squad.json')\n",
    "convert_squad(train_data, file_name = store_dir + 'train_data_squad.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNvrREUFuPsI"
   },
   "outputs": [],
   "source": [
    "## ref https://github.com/google-research/bert\n",
    "#####################################################\n",
    "## use bert to fine tune and predict on our data   ##\n",
    "#####################################################\n",
    "\n",
    "!python run_squad.py  \\\n",
    "    --vocab_file={/chinese_L-12_H-768_A-12/vocab.txt} \\\n",
    "    --bert_config_file={/chinese_L-12_H-768_A-12/bert_config.json} \\\n",
    "    --init_checkpoint={/chinese_L-12_H-768_A-12/bert_model.ckpt} \\\n",
    "    --do_train = True \\\n",
    "    --train_file={path to sougou.json} \\\n",
    "    --do_predict= False \\\n",
    "    --predict_file= {path to our data (test_data_squad/train_data_squad.json)} \\\n",
    "    --train_batch_size=12 --num_train_epochs=2.0 \\\n",
    "    --max_seq_length=384 \\\n",
    "    --doc_stride=128 \\\n",
    "    --learning_rate=3e-5 \\\n",
    "    --save_checkpoints_steps=1000 \\\n",
    "    --output_dir=./output/distant_supervision_full \\\n",
    "    --do_lower_case=True \\\n",
    "    --use_tpu=False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvH2CQEQxIDs"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## pickle the bert output ##\n",
    "############################\n",
    "\n",
    "def pickle_answer(input_data, pred_file = \"dev_predictions_test.json\",\\\n",
    "                  pred_file_nbest = \"dev_nbest_predictions_test.json\",\\\n",
    "                  file_name_output = \"pred_train_bert.pkl\", saving_path = \"./preprocessed_data/\"):\n",
    "  \n",
    "  \n",
    "  with open(pred_file, \"r\") as read_file:\n",
    "      dev_data = json.load(read_file)\n",
    "\n",
    "  with open(pred_file_nbest, \"r\") as read_file:\n",
    "      dev_data_nbest = json.load(read_file)\n",
    "\n",
    "  pred_output = {}\n",
    "  for i in range(len(input_data)):\n",
    "    tmp = []\n",
    "    for j in range(len(input_data[i]['corpus'])):\n",
    "      dev_id = 'Trainbaidu_%d_context_%d' %(i, j)\n",
    "      try:\n",
    "        tmp.append( [ [dev_data[dev_id]], [dev_data_nbest[dev_id][0]['probability']] ])\n",
    "      except:\n",
    "        tmp.append( [ [\"ERROR\"], [0] ])\n",
    "    pred_output[input_data[i]['id']] = tmp\n",
    "  with open(saving_path + file_name_output, 'wb') as f:\n",
    "    pickle.dump(pred_output, f)\n",
    "  \n",
    "saving_path = \"./preprocessed_data/\"\n",
    "bert_output_path = \"./bert_output/\"\n",
    "\n",
    "train_data = pickle.load(open(\"./preprocessed_data/train_data.pkl\", \"rb\" ))\n",
    "test_data = pickle.load(open(\"./preprocessed_data/test_data.pkl\", \"rb\" ))\n",
    "\n",
    "pickle_answer(input_data = test_data, pred_file = bert_output_path + \"dev_predictions_test.json\",\\\n",
    "              pred_file_nbest = bert_output_path + \"dev_nbest_predictions_test.json\",\\\n",
    "              file_name_output = \"pred_test_bert.pkl\", saving_path = saving_path)\n",
    "\n",
    "pickle_answer(input_data = train_data, pred_file = bert_output_path + \"dev_predictions_train.json\",\\\n",
    "              pred_file_nbest = bert_output_path + \"dev_nbest_predictions_train.json\",\\\n",
    "              file_name_output = \"pred_train_bert.pkl\", saving_path = saving_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMr98X1VsdWA"
   },
   "source": [
    "### QANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 32395,
     "status": "ok",
     "timestamp": 1602967424110,
     "user": {
      "displayName": "Sheng Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhraDPO3yxolOf82I55EVZaIAfn4g1oUU6mSS7k=s64",
      "userId": "16336041814041002028"
     },
     "user_tz": 240
    },
    "id": "QKs8AN9VseoO",
    "outputId": "64dd993b-ca49-4d31-f73c-084c25b92662"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qanet_mixall_alter0.h5\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained QANet\n",
    "os.chdir(\"./qanet_model\")\n",
    "import qanet as net\n",
    "from databunch import *\n",
    "LoadModel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvWQK3nbxWpg"
   },
   "outputs": [],
   "source": [
    "def find_s_list(data, train = True):\n",
    "  if train:\n",
    "    s_list = set()\n",
    "    for i in range(len(data)):\n",
    "      s_list.add(data[i]['s'])\n",
    "    s_list = list(s_list)\n",
    "  else:\n",
    "    s_list = set()\n",
    "    for i in range(len(data)):\n",
    "      s_list.add(data[i][0])\n",
    "    s_list = list(s_list)\n",
    "  return s_list\n",
    "\n",
    "def GetAnswerListNew2(db_test, y_pred_start, y_pred_end, listans=True, addscore=[]):\n",
    "    def GetAnswer(i, start, end):\n",
    "        return ''.join(db_test.contextRaw[i][start:end + 1])\n",
    "    answer = []\n",
    "    predAnswerList = []\n",
    "    for ii in range(db_test.numQuestions):\n",
    "        cs, cspos = {}, {}\n",
    "        for i in range(db_test.startEnd[ii][0], db_test.startEnd[ii][1] + 1):\n",
    "            thisMax = [-1e+5] ;\n",
    "            canswer = [\"NaN**\"] \n",
    "            for j1 in range(maxPLen):\n",
    "                for j2 in range(j1, min(maxPLen, j1 + 8)):\n",
    "                    score = y_pred_start[i][j1] * y_pred_end[i][j2]\n",
    "                    if score > min(thisMax) or canswer == \"NaN**\":\n",
    "                        temp = GetAnswer(i, j1, j2)\n",
    "                        if temp in db_test.questionRaw[ii]: continue\n",
    "                        canswer[np.argmin(thisMax)] = temp\n",
    "                        thisMax[np.argmin(thisMax)] = score                        \n",
    "                        mj1, mj2 = j1, j2\n",
    "            # one passage\n",
    "            if listans:\n",
    "                tlen = len(db_test.contextRaw[i])\n",
    "                if mj2 + 1 < tlen and db_test.contextRaw[i][mj2 + 1] == '、':\n",
    "                    jj2 = mj2 + 1\n",
    "                    while jj2 < tlen:\n",
    "                        token = db_test.contextRaw[i][jj2];\n",
    "                        jj2 += 1\n",
    "                        #print(token)\n",
    "                        if token[0] in '，。,. 被是': break\n",
    "                        canswer += token;\n",
    "                        mj2 += 1\n",
    "            answer.append([canswer, thisMax])\n",
    "    return answer\n",
    "\n",
    "def GetCandidateAnswer(zz, query):\n",
    "    ret = {'answer': '@NULL@', 'query': query, 'query_id': '0'}\n",
    "    passages = []\n",
    "    for text in zz:\n",
    "        z = {'url': '', 'passage_text': text}\n",
    "        passages.append(z)\n",
    "    ret['passages'] = passages\n",
    "    db = DataBunch(None, False, onejson=ret)\n",
    "    X, Y = db.GetData()\n",
    "    y_pred_start, y_pred_end = net.mm.predict(X, batch_size=128)  # ;print(y_pred_start,y_pred_end)\n",
    "    corpus_addscore = [0.5, 0.5]\n",
    "    isanslist = False\n",
    "    ret = GetAnswerListNew2(db, y_pred_start, y_pred_end, isanslist, addscore=corpus_addscore)\n",
    "    # print(ret)\n",
    "    return ret\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMdpY5h8xblU"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/drive/My Drive/Knowledge Extraction/szhang37_code/KG_RL\")\n",
    "train_data = pickle.load(open(\"./preprocessed_data/train_data.pkl\", \"rb\" ))\n",
    "test_data = pickle.load(open(\"./preprocessed_data/test_data.pkl\", \"rb\" ))\n",
    "test_data_types = [test_data[i]['type'] for i in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 39102,
     "status": "ok",
     "timestamp": 1602967463261,
     "user": {
      "displayName": "Sheng Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhraDPO3yxolOf82I55EVZaIAfn4g1oUU6mSS7k=s64",
      "userId": "16336041814041002028"
     },
     "user_tz": 240
    },
    "id": "K4_542ul0Rtn",
    "outputId": "b9a83a0b-d250-4be6-e313-9b6ae5a5246a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.741 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_pred = {}\n",
    "for i in range(len(train_data)):\n",
    "  key = 'Train_'+ str(i)\n",
    "  entry = train_data[i]\n",
    "  text_list = entry['corpus']\n",
    "  query = entry['s'] + ' ' + entry['p']\n",
    "  answer_list = GetCandidateAnswer(text_list, query)\n",
    "  train_pred[key] = answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10cYqm6hysZG"
   },
   "outputs": [],
   "source": [
    "test_pred = {}\n",
    "for i in range(len(test_data)):\n",
    "  key = 'Test_'+ str(i)\n",
    "  entry = test_data[i]\n",
    "  text_list = entry['corpus']\n",
    "  query = entry['s'] + ' ' + entry['p']\n",
    "  answer_list = GetCandidateAnswer(text_list, query)\n",
    "  test_pred[key] = answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfhLw-3q0Zqt"
   },
   "outputs": [],
   "source": [
    "saving_path = \"./preprocessed_data/\"\n",
    "with open(saving_path+\"pred_train_qanet.pkl\", 'wb') as f:\n",
    "  pickle.dump(train_pred, f)\n",
    "with open(saving_path+\"pred_test_qanet.pkl\", 'wb') as f:\n",
    "  pickle.dump(test_pred, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Data_preparation.ipynb",
   "provenance": [
    {
     "file_id": "1B4Tini7F3Tv_nyS8CPjm7nFz7MBclEmk",
     "timestamp": 1602484297078
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
